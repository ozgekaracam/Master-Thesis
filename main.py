# This is a sample Python script.

# Press ⌃R to execute it or replace it with your code.
# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.
#Import pandas
import pandas as pd
import time
import re
import string
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
import nltk
from nltk.corpus import wordnet
from collections import Counter
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from mlxtend.frequent_patterns import fpgrowth
from d3graph import d3graph, vec2adjmat
from d3blocks import D3Blocks
from io import StringIO
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from urllib.request import urlopen


import numpy as np

def visualize_graph(apriori_rules):
    # Initialize
    d3 = d3graph()

    #df = pd.DataFrame(data=apriori_rules, columns=['source', 'target', 'weight'])
    #d3.d3graph(df, showfig=False)
    #d3.D3graph.set_node_properties(color='cluster')

    adjmat = vec2adjmat(apriori_rules['antecedents'], apriori_rules['consequents'], weight=apriori_rules['lift'])
    d3.graph(adjmat)
    size = []
    d3.set_edge_properties(directed=True)

    size = [30 if x in concern_list else 10 for x in adjmat.columns.astype(str)]
    edge_size = [5 if x in app_list else 1 for x in adjmat.columns.astype(str)]
    d3.set_node_properties(color='cluster', size=size, edge_size=edge_size)

    d3.set_edge_properties(directed=True)
    # , scaler='minmax'
    # d3.set_edge_properties() , edge_distance = 0
    #d3.set_node_properties(color=adjmat.columns.values)
    # Show
    d3.show()
# Generate rules
def generate_rules(frequent_itemsets):
    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=20)
    rules.sort_values('confidence', ascending=False, inplace=True)
    # Set minimum antecedent support
    #rules = rules[rules['antecedent support'] > 0.01]
    # Set maximum consequent support
    #rules = rules[apriori_rules['consequent support'] < 0.1]
    #rules = rules[rules['confidence'] > 0.001]
    rules.antecedents = rules.antecedents.apply(lambda x: next(iter(x)))
    rules.consequents = rules.consequents.apply(lambda x: next(iter(x)))
    return rules
# One-hot encoder
def encode_onehot(corpus_list):
    te = TransactionEncoder()
    te_ary = te.fit(corpus_list).transform(corpus_list)
    corpus_df = pd.DataFrame(te_ary, columns=te.columns_)
    return corpus_df
# Apriori algorithm
def apriori_algo(corpus_list):
    start_time = time.time()
    frequent_itemsets = apriori(encode_onehot(corpus_list), min_support=0.003, use_colnames=True, low_memory=True)
    print("---Runtime Apriori: %s seconds ---" % (time.time() - start_time))
    #API: apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False)
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
    print("the number of frequent itemsets generated by Apriori:", len(frequent_itemsets))
    rules = generate_rules(frequent_itemsets)
    print("number of rules generated by Apriori: ", len(rules))
    return rules
# FP Growth algoritm
def fpgrowth_algo(corpus_list):
    start_time = time.time()
    frequent_itemsets = fpgrowth(encode_onehot(corpus_list), min_support=0.003, use_colnames=True)
    print("---Runtime FP Growth: %s seconds ---" % (time.time() - start_time))
    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
    print("the number of frequent itemsets generated by FP Growth:", len(frequent_itemsets))
    rules = generate_rules(frequent_itemsets)
    print("number of rules generated by FP Growth: ", len(rules))
    return rules

app_list = []
concern_list = []
# Create corpus list
def create_corpus(df,  concern: bool = False, app: bool = False):
    #df = dataset.query("aacat1 not in ['Noise', 'none']")
    df['clean_content'] = df.clean_content.apply(lambda x: x.split(' '))
    # df['aacat1'].unique()
    #cleanContent = df['clean_content']
    if concern:
        for index, row in df.iterrows():
            row["clean_content"].append(row["predicted"])
            if row["predicted"] not in concern_list: concern_list.append(row["predicted"])
    if app:
        for index, row in df.iterrows():
            row["clean_content"].append(row["aapp_name"])
            app_list.append(row["aapp_name"])
    corpus_list = df["clean_content"].tolist()
    return corpus_list

def word_stemmer(stemmer, text):
    stem_text = [stemmer.stem(i) for i in text]
    return stem_text
def get_part_of_speech(word):
    probable_part_of_speech = wordnet.synsets(word)
    pos_counts = Counter()
    pos_counts["n"] = len([item for item in probable_part_of_speech if item.pos() == "n"])
    pos_counts["v"] = len([item for item in probable_part_of_speech if item.pos() == "v"])
    pos_counts["a"] = len([item for item in probable_part_of_speech if item.pos() == "a"])
    pos_counts["r"] = len([item for item in probable_part_of_speech if item.pos() == "r"])

    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]
    return most_likely_part_of_speech
def word_lemmatizer(lemmatizer, text):
    lem_text = [lemmatizer.lemmatize(i, get_part_of_speech(i)) for i in text]
    return lem_text

def removeNumber(text):
    return ' '.join(re.sub(r'[0-9]',' ', text).split())
def deEmojify(text):
    return text.encode('ascii', 'ignore').decode('ascii')
def generate_stopwords():
    # http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop
    # stpwrd = nltk.corpus.stopwords.words('english')
    target_url = "http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop"
    response = urlopen(target_url)
    stpwrd = response.read().decode('utf-8').replace("\n", " ").split()
    new_stopwords = ["people", "app", "im", "it", "me"]
    stpwrd.extend(new_stopwords)
    return stpwrd
stpwrd = generate_stopwords()
def remove_stopwords(text):
    text = text.split(" ")
    words = [w for w in text if w not in stpwrd]
    return ' '.join(words)
def removePunctuation(text):
    no_punc = "".join([c for c in text if c not in string.punctuation])
    return no_punc
def removeLink(text):
    no_link = ' '.join(re.sub("(w+://S+)", " ", text).split())
    return no_link
def preprocess(content): #res -> clean_content
    clean_content = content.lower()
    # removeLinks
    clean_content = removeLink(clean_content)
    # remove stop words
    clean_content = remove_stopwords(clean_content)
    # removePunc
    clean_content = removePunctuation(clean_content)
    # removeEmojis
    clean_content = deEmojify(clean_content)
    # removeNumber
    clean_content = removeNumber(clean_content)
    # tokenizer
    tokenizer = RegexpTokenizer(r'\w+|\$[\d\.]+|\S+')
    clean_content = tokenizer.tokenize(clean_content)
    # lemmatizer
    lemmatizer = WordNetLemmatizer()
    clean_content = word_lemmatizer(lemmatizer, clean_content)
    # stemmer
    #stemmer = PorterStemmer()
    #clean_content = word_stemmer(stemmer, clean_content)
    return ' '.join(clean_content)
def prepare_data(dataset):
#prepare_data(df, stemmer='lan', spellcheck=False):
    start_time = time.time()
    dataset['clean_content'] = [preprocess(x) for x in dataset['content']]
    #if spellcheck: df.to_csv("/Users/neel/Desktop/bigsample_spellchecked.csv")
    print("--- %s seconds ---" % (time.time() - start_time))
    return dataset
def get_data(file):
    data = pd.read_csv(file)
    print(data.shape)
    return data
# Press the green button in the gutter to run the script.
def factorize_concern(dataset):
    dataset['aacat1_id'] = dataset['aacat1'].factorize()[0]
    concern_id_df = dataset[['aacat1', 'aacat1_id']].drop_duplicates().sort_values('aacat1_id')
    concern_to_id = dict(concern_id_df.values)
    id_to_concern = dict(concern_id_df[['aacat1_id', 'aacat1']].values)
    return dataset, concern_id_df, concern_to_id, id_to_concern
def clean_no_concern(dataset):
    df = dataset[pd.notnull(dataset['clean_content'])]
    df = df.query("aacat1 not in ['Noise', 'Other', 'none']")
    # get a function here to define top
    df_count = df.groupby('aacat1').clean_content.count().reset_index(name='counts')
    top_list = df_count[df_count['counts'] > 50]['aacat1']
    top_list = top_list.to_list()
    dataset = dataset.query("aacat1  in @top_list")
    print(dataset.groupby('aacat1').clean_content.count())
    return dataset
def vectorizer(df):
    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))
    features = tfidf.fit_transform(df.clean_content).toarray()
    labels = df.aacat1_id
    print("features: ", features.shape)
    return  features, labels
def run_models(dataset,features, labels):
    modelSVC = LinearSVC()
    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, dataset.index,
                                                                                     test_size=0.33, random_state=0)
    modelSVC.fit(X_train, y_train)
    y_pred = modelSVC.predict(features)
    return y_pred
def add_predict(df, predictions):
    df['predicted_id'] = predictions
    df['predicted'] = [id_to_concern[x] for x in df['predicted_id']]
    return df
if __name__ == '__main__':
    file = "final_annotations.csv"
    # prepare the data
    print("preprocessing data...")
    dataset = prepare_data(get_data(file))
    print("done preprocessing data...")
    print("dataset length: ", len(dataset))
    dataset, concern_id_df, concern_to_id, id_to_concern = factorize_concern(dataset)
    dataset = clean_no_concern(dataset)
    features, labels = vectorizer(dataset)
    y_pred = run_models(dataset, features, labels)
    predict_dataset = add_predict(dataset, y_pred)
    corpus_list = create_corpus(predict_dataset, concern=True, app=True)
    apriori_rules = apriori_algo(corpus_list)
    #fpgrowth_rules = fpgrowth_algo(corpus_list)
    visualize_graph(apriori_rules)
# See PyCharm help at https://www.jetbrains.com/help/pycharm/
