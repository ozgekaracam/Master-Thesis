{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69350639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample Python script.\n",
    "\n",
    "# Press ⌃R to execute it or replace it with your code.\n",
    "# Press Double ⇧ to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "#Import pandas\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "#Instantiate Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "def word_stemmer(text):\n",
    "    stem_text = [stemmer.stem(i) for i in text]\n",
    "    return stem_text\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos() == \"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos() == \"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos() == \"r\"])\n",
    "\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "# Instantiate lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i, get_part_of_speech(i)) for i in text]\n",
    "    return lem_text\n",
    "\n",
    "def removeNumber(text):\n",
    "    return' '.join(re.sub(r'[0-9]',' ', text).split())\n",
    "def deEmojify(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "def remove_stopwords(text):\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    text = text.split(\" \")\n",
    "    words = [w for w in text if w not in stpwrd]\n",
    "    return ' '.join(words)\n",
    "def removePunctuation(text):\n",
    "    no_punc = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punc\n",
    "def removeLink(text):\n",
    "    no_link = ' '.join(re.sub(\"(w+://S+)\", \" \", text).split())\n",
    "    return no_link\n",
    "def preprocess(content): #res -> clean_content\n",
    "    clean_content = content.lower()\n",
    "    # removeLinks\n",
    "    clean_content = removeLink(clean_content)\n",
    "    # remove stop words\n",
    "    clean_content = remove_stopwords(clean_content)\n",
    "    # removePunc\n",
    "    clean_content = removePunctuation(clean_content)\n",
    "    # removeEmojis\n",
    "    clean_content = deEmojify(clean_content)\n",
    "    # removeNumber\n",
    "    clean_content = removeNumber(clean_content)\n",
    "    # tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    clean_content = tokenizer.tokenize(clean_content)\n",
    "    # lemmatizer\n",
    "    clean_content = word_lemmatizer(clean_content)\n",
    "    # stemmer\n",
    "    #clean_content = word_stemmer(clean_content)\n",
    "    return clean_content\n",
    "def prepare_data(dataset):\n",
    "#prepare_data(df, stemmer='lan', spellcheck=False):\n",
    "    start_time = time.time()\n",
    "    dataset['clean_content'] = [preprocess(x) for x in dataset['content']]\n",
    "    #if spellcheck: df.to_csv(\"/Users/neel/Desktop/bigsample_spellchecked.csv\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return dataset\n",
    "def get_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029c05e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data...\n",
      "(3097, 16)\n",
      "--- 4.81927490234375 seconds ---\n",
      "done preprocessing data...\n",
      "3097\n"
     ]
    }
   ],
   "source": [
    "file = \"final_annotations.csv\"\n",
    "# prepare the data\n",
    "print(\"preprocessing data...\")\n",
    "dataset = prepare_data(get_data(file))\n",
    "print(\"done preprocessing data...\")\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.query(\"aacat1 not in ['Noise', 'none']\")\n",
    "#df['aacat1'].unique()\n",
    "corpus_list = df['clean_content'].tolist()\n",
    "#corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56144425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aap</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbreviate</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ableism</th>\n",
       "      <th>ableist</th>\n",
       "      <th>aboslutely</th>\n",
       "      <th>...</th>\n",
       "      <th>yu</th>\n",
       "      <th>zac</th>\n",
       "      <th>zayanbutt</th>\n",
       "      <th>zero</th>\n",
       "      <th>zindabad</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomyesterday</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1922 rows × 4805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a     aa    aap     ab  abbreviate  ability   able  ableism  \\\n",
       "0     False  False  False  False       False    False  False    False   \n",
       "1     False  False  False  False       False    False  False    False   \n",
       "2     False  False  False  False       False    False  False    False   \n",
       "3     False  False  False  False       False    False  False    False   \n",
       "4     False  False  False  False       False    False  False    False   \n",
       "...     ...    ...    ...    ...         ...      ...    ...      ...   \n",
       "1917  False  False  False  False       False    False  False    False   \n",
       "1918  False  False  False  False       False    False  False    False   \n",
       "1919  False  False  False  False       False    False  False    False   \n",
       "1920  False  False  False  False       False    False  False    False   \n",
       "1921  False  False  False  False       False    False  False    False   \n",
       "\n",
       "      ableist  aboslutely  ...     yu    zac  zayanbutt   zero  zindabad  \\\n",
       "0       False       False  ...  False  False      False  False     False   \n",
       "1       False       False  ...  False  False      False  False     False   \n",
       "2       False       False  ...  False  False      False  False     False   \n",
       "3       False       False  ...  False  False      False  False     False   \n",
       "4       False       False  ...  False  False      False  False     False   \n",
       "...       ...         ...  ...    ...    ...        ...    ...       ...   \n",
       "1917    False       False  ...  False  False      False  False     False   \n",
       "1918    False       False  ...  False  False      False  False     False   \n",
       "1919    False       False  ...  False  False      False  False     False   \n",
       "1920    False       False  ...  False  False      False  False     False   \n",
       "1921    False       False  ...  False  False      False  False     False   \n",
       "\n",
       "       zone   zoom  zoomyesterday   zuck  zuckerberg  \n",
       "0     False  False          False  False       False  \n",
       "1     False  False          False  False       False  \n",
       "2     False  False          False  False       False  \n",
       "3     False  False          False  False       False  \n",
       "4     False  False          False  False       False  \n",
       "...     ...    ...            ...    ...         ...  \n",
       "1917  False  False          False  False       False  \n",
       "1918  False  False          False  False       False  \n",
       "1919  False  False          False  False       False  \n",
       "1920  False  False          False  False       False  \n",
       "1921  False   True          False  False       False  \n",
       "\n",
       "[1922 rows x 4805 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Apriori algorithm, this dataset needs to be one-hot encoded.\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(corpus_list).transform(corpus_list)\n",
    "corpus_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule = apriori(transactions = corpus_list, min_support = 0.005, min_confidence = 0.05, min_lift = 3, min_length = 2, max_length = 2)\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import time\n",
    "start_time = time.time()\n",
    "frequent_itemsets = apriori(corpus_df, min_support = 0.001, use_colnames=True)\n",
    "print(\"---Runtime: %s seconds ---\" % (time.time() - start_time))\n",
    "#API: apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "print(\"the number of frequent itemsets generated:\", len(frequent_itemsets))\n",
    "frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27124acf",
   "metadata": {},
   "source": [
    "1. Support - It is the measure of frequency or abundance of an item in a dataset. It can be 'antecedent support', 'consequent support', and 'support'. 'antecedent support' contains proportion of transactions done for the antecedent while 'consequent support' involves those for consequent. 'Support' is computed for both antecedent and consequent in question.\n",
    "2. Confidence - This gives the probability of consequent in a transaction given the presence of antecedent.\n",
    "3. Lift - Given that antecedents and consequents are independent, how often do they come together/bought together.\n",
    "4. Leverage - It is the difference between frequency of antecedent and consequent together in transactions to frequency of both in independent transactions.\n",
    "5. Conviction - A higher conviction score means that consequent is highly dependent on antecedent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd07007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=9)\n",
    "#rules.antecedents = rules.antecedents.apply(lambda x: next(iter(x)))\n",
    "#rules.consequents = rules.consequents.apply(lambda x: next(iter(x)))\n",
    "rules\n",
    "#display(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatterplot using support and confidence\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns1\n",
    "plt.figure(figsize=(10,6))\n",
    "sns1.scatterplot(x = \"support\", y = \"confidence\", data = rules)\n",
    "plt.margins(0.01,0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scatterplot using support and confidence\n",
    "plt.figure(figsize=(10,6))\n",
    "sns1.scatterplot(x = \"support\", y = \"confidence\", \n",
    "                size = \"lift\", data = rules)\n",
    "plt.margins(0.01,0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "import seaborn as sns1\n",
    "rules_top = rules.nlargest(n = 20, columns = 'lift')\n",
    "\n",
    "pivot = rules_top.pivot(index = 'consequents', \n",
    "                    columns = 'antecedents', values= 'lift')\n",
    "\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "plt.figure(figsize=(10,6))\n",
    "sns1.heatmap(pivot, annot = True, cbar = False)\n",
    "b, t = plt.ylim() \n",
    "b += 0.5 \n",
    "t -= 0.5 \n",
    "plt.ylim(b, t) \n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform antecedent, consequent, and support columns into matrix\n",
    "support_table = rules_top.pivot(index='consequents', columns='antecedents', values='support')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns1.heatmap(support_table, annot=True, cbar=False)\n",
    "b, t = plt.ylim() \n",
    "b += 0.5 \n",
    "t -= 0.5 \n",
    "plt.ylim(b, t) \n",
    "plt.yticks(rotation=0)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=9)\n",
    "rules.antecedents = rules.antecedents.apply(lambda x: next(iter(x)))\n",
    "rules.consequents = rules.consequents.apply(lambda x: next(iter(x)))\n",
    "rules_top = rules.nlargest(n = 30, columns = 'lift')\n",
    "\n",
    "import networkx as nx\n",
    "fig, ax=plt.subplots(figsize=(10,4))\n",
    "GA=nx.from_pandas_edgelist(rules_top,source='antecedents',target='consequents', create_using = nx.MultiDiGraph(), edge_attr='lift')\n",
    "\n",
    "nx.draw(GA, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f375a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
